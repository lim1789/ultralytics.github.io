{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction To get started right now check out the Quick Start Guide What is YOLOv5 YOLO an acronym for 'You only look once', is an object detection algorithm that divides images into a grid system. Each cell in the grid is responsible for detecting objects within itself. YOLO is one of the most famous object detection algorithms due to its speed and accuracy. The History of YOLO YOLOv5 Shortly after the release of YOLOv4 Glenn Jocher introduced YOLOv5 using the Pytorch framework. The open source code is available on GitHub Author: Glenn Jocher Released: 18 May 2020 YOLOv4 With the original authors work on YOLO coming to a standstill, YOLOv4 was released by Alexey Bochoknovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. The paper was titled YOLOv4: Optimal Speed and Accuracy of Object Detection Author: Alexey Bochoknovskiy , Chien-Yao Wang , and Hong-Yuan Mark Liao Released: 23 April 2020 YOLOv3 YOLOv3 improved on the YOLOv2 paper and both Joseph Redmon and Ali Farhadi, the original authors, contributed. Together they published YOLOv3: An Incremental Improvement The original YOLO papers were are hosted here Author: Joseph Redmon and Ali Farhadi Released: 8 Apr 2018 YOLOv2 YOLOv2 was a joint endevor by Joseph Redmon the original author of YOLO and Ali Farhadi. Together they published YOLO9000:Better, Faster, Stronger Author: Joseph Redmon and Ali Farhadi Released: 25 Dec 2016 YOLOv1 YOLOv1 was released as a research paper by Joseph Redmon. The paper was titled You Only Look Once: Unified, Real-Time Object Detection Author: Joseph Redmon Released: 8 Jun 2015","title":"Introduction"},{"location":"#introduction","text":"To get started right now check out the Quick Start Guide","title":"Introduction"},{"location":"#what-is-yolov5","text":"YOLO an acronym for 'You only look once', is an object detection algorithm that divides images into a grid system. Each cell in the grid is responsible for detecting objects within itself. YOLO is one of the most famous object detection algorithms due to its speed and accuracy.","title":"What is YOLOv5"},{"location":"#the-history-of-yolo","text":"","title":"The History of YOLO"},{"location":"#yolov5","text":"Shortly after the release of YOLOv4 Glenn Jocher introduced YOLOv5 using the Pytorch framework. The open source code is available on GitHub Author: Glenn Jocher Released: 18 May 2020","title":"YOLOv5"},{"location":"#yolov4","text":"With the original authors work on YOLO coming to a standstill, YOLOv4 was released by Alexey Bochoknovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. The paper was titled YOLOv4: Optimal Speed and Accuracy of Object Detection Author: Alexey Bochoknovskiy , Chien-Yao Wang , and Hong-Yuan Mark Liao Released: 23 April 2020","title":"YOLOv4"},{"location":"#yolov3","text":"YOLOv3 improved on the YOLOv2 paper and both Joseph Redmon and Ali Farhadi, the original authors, contributed. Together they published YOLOv3: An Incremental Improvement The original YOLO papers were are hosted here Author: Joseph Redmon and Ali Farhadi Released: 8 Apr 2018","title":"YOLOv3"},{"location":"#yolov2","text":"YOLOv2 was a joint endevor by Joseph Redmon the original author of YOLO and Ali Farhadi. Together they published YOLO9000:Better, Faster, Stronger Author: Joseph Redmon and Ali Farhadi Released: 25 Dec 2016","title":"YOLOv2"},{"location":"#yolov1","text":"YOLOv1 was released as a research paper by Joseph Redmon. The paper was titled You Only Look Once: Unified, Real-Time Object Detection Author: Joseph Redmon Released: 8 Jun 2015","title":"YOLOv1"},{"location":"quick-start/","text":"Getting Started Requirements You will need Python >= 3.8 and PIP in order to follow this guide. The rest of the requirements are listed in './requirements.txt' * If you have mutliple versions of python installed, ensure you are using the correct one Installation Clone the repository $ git clone https://github.com/ultralytics/yolov5.git Enter the repository root directory $ cd yolov5 Install the required packages from your cloned repository root directory $ pip install -r requirements.txt Packaged Environments For a quick and hassle free setup YOLOv5 has been packaged with all dependencies* for the following environments *including CUDA / CUDNN , Python and PyTorch Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Inference - Detect Objects From Your Cloned Repository To get started with object detection using the latest YOLO models , run this command from your repository root directory. Results are saved to './runs/detect' $ python detect.py --source OPTION Replace OPTION with your selection, to detect from: Webcam : (OPTION = 0) For live object detection from your connected webcam Image : (OPTION = filename.jpg) Create a copy of the image with an object detection overlay Video : (OPTION = filename.mp4) Create a copy of the video with an object detection overlay Directory : (OPTION = directory_name/) Create a copy of all file with an object detection overlay Global File Type (OPTION = directory_name/*.jpg) Create a copy of all file with an object detection overlay RTSP stream : (OPTION = rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa) For live object detection from a stream RTMP stream : (OPTION = rtmp://192.168.1.105/live/test) For live object detection from a stream HTTP stream : (OPTION = http://112.50.243.8/PLTV/88888888/224/3221225900/1.m3u8) For live object detection from a stream The following file formats are currently supported: Images: bmp, jpg, jpeg, png, tif, tiff, dng, webp, mpo Videos: mov, avi, mp4, mpg, mpeg, m4v, wmv, mkv From PyTorch Hub Inference can be run directly from PyTorch Hub without cloning the repository. The necesary files will be downloaded into your temporary directory. Here is an example script that uses the latest YOLOv5s model and the repositories example images. import torch # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Images dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/' imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')] # batch of images # Inference results = model(imgs) results.print() # or .show(), .save() Training Models Check out the tutorial section for guides on training custom models","title":"Quick Start"},{"location":"quick-start/#getting-started","text":"","title":"Getting Started"},{"location":"quick-start/#requirements","text":"You will need Python >= 3.8 and PIP in order to follow this guide. The rest of the requirements are listed in './requirements.txt' * If you have mutliple versions of python installed, ensure you are using the correct one","title":"Requirements"},{"location":"quick-start/#installation","text":"Clone the repository $ git clone https://github.com/ultralytics/yolov5.git Enter the repository root directory $ cd yolov5 Install the required packages from your cloned repository root directory $ pip install -r requirements.txt","title":"Installation"},{"location":"quick-start/#packaged-environments","text":"For a quick and hassle free setup YOLOv5 has been packaged with all dependencies* for the following environments *including CUDA / CUDNN , Python and PyTorch Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Packaged Environments"},{"location":"quick-start/#inference-detect-objects","text":"","title":"Inference - Detect Objects"},{"location":"quick-start/#from-your-cloned-repository","text":"To get started with object detection using the latest YOLO models , run this command from your repository root directory. Results are saved to './runs/detect' $ python detect.py --source OPTION Replace OPTION with your selection, to detect from: Webcam : (OPTION = 0) For live object detection from your connected webcam Image : (OPTION = filename.jpg) Create a copy of the image with an object detection overlay Video : (OPTION = filename.mp4) Create a copy of the video with an object detection overlay Directory : (OPTION = directory_name/) Create a copy of all file with an object detection overlay Global File Type (OPTION = directory_name/*.jpg) Create a copy of all file with an object detection overlay RTSP stream : (OPTION = rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa) For live object detection from a stream RTMP stream : (OPTION = rtmp://192.168.1.105/live/test) For live object detection from a stream HTTP stream : (OPTION = http://112.50.243.8/PLTV/88888888/224/3221225900/1.m3u8) For live object detection from a stream The following file formats are currently supported: Images: bmp, jpg, jpeg, png, tif, tiff, dng, webp, mpo Videos: mov, avi, mp4, mpg, mpeg, m4v, wmv, mkv","title":"From Your Cloned Repository"},{"location":"quick-start/#from-pytorch-hub","text":"Inference can be run directly from PyTorch Hub without cloning the repository. The necesary files will be downloaded into your temporary directory. Here is an example script that uses the latest YOLOv5s model and the repositories example images. import torch # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Images dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/' imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')] # batch of images # Inference results = model(imgs) results.print() # or .show(), .save()","title":"From PyTorch Hub"},{"location":"quick-start/#training-models","text":"Check out the tutorial section for guides on training custom models","title":"Training Models"},{"location":"tutorials/latest/","text":"Latest Tutorials","title":"Latest"},{"location":"tutorials/latest/#latest-tutorials","text":"","title":"Latest Tutorials"},{"location":"tutorials/recommended/","text":"Recomended Tutorials Train Custom Models Training Tips for Best Results","title":"Recommended"},{"location":"tutorials/recommended/#recomended-tutorials","text":"Train Custom Models Training Tips for Best Results","title":"Recomended Tutorials"},{"location":"tutorials/tutorials/","text":"All Tutorials Install and Setup GitHub Clone Google Cloud Platform Amazon Web Services Docker PyTorch Hub Training Models Train Custom Models Training Tips for Best Results Training with Multiple GPU's Logging and Analytics with Weights and Biases The Supervisely Ecosystem PyTorch Hub ONNX and TorchScript Export Test-Time Augmentation (TTA) Model Ensembling Model Pruning/Sparsity Hyperparameter Evolution Transfer Learning with Frozen Layers \u2b50 NEW [TensorRT Deployment](https:/","title":"Categories"},{"location":"tutorials/tutorials/#all-tutorials","text":"","title":"All Tutorials"},{"location":"tutorials/tutorials/#install-and-setup","text":"GitHub Clone Google Cloud Platform Amazon Web Services Docker PyTorch Hub","title":"Install and Setup"},{"location":"tutorials/tutorials/#training-models","text":"Train Custom Models Training Tips for Best Results Training with Multiple GPU's","title":"Training Models"},{"location":"tutorials/tutorials/#logging-and-analytics-with-weights-and-biases","text":"","title":"Logging and Analytics with Weights and Biases"},{"location":"tutorials/tutorials/#the-supervisely-ecosystem","text":"","title":"The Supervisely Ecosystem"},{"location":"tutorials/tutorials/#pytorch-hub","text":"ONNX and TorchScript Export Test-Time Augmentation (TTA) Model Ensembling Model Pruning/Sparsity Hyperparameter Evolution Transfer Learning with Frozen Layers \u2b50 NEW [TensorRT Deployment](https:/","title":"PyTorch Hub"},{"location":"tutorials/articles/AWS-Quickstart/","text":"This quickstart guide helps new users run YOLOv5 \ud83d\ude80 on an Amazon Web Services (AWS) Deep Learning instance \u2b50. AWS offers a Free Tier and a credit program to get started quickly and affordably. Other quickstart options for YOLOv5 include our Colab Notebook , GCP Deep Learning VM and our Docker image at https://hub.docker.com/r/ultralytics/yolov5 . 1. Console Sign-in Create and account or sign-in to the AWS console at https://aws.amazon.com/console/ and then select the EC2 service. 2. Launch Instance In the EC2 part of the AWS console, click the Launch instance button. Choose an Amazon Machine Image (AMI) Enter 'Deep Learning' in the search field and select the most recent Ubuntu Deep Learning AMI (recommended), or select an alternative Deep Learning AMI. See Choosing Your DLAMI for more information on selecting an AMI. Select an Instance Type A GPU instance is recommended for most deep learning purposes. Training new models will be faster on a GPU instance than a CPU instance. You can scale sub-linearly when you have multi-GPU instances or if you use distributed training across many instances with GPUs. To set up distributed training, see Distrbuted Training . Note: The size of your model should be a factor in selecting an instance. If your model exceeds an instance's available RAM, select a different instance type with enough memory for your application. Amazon EC2 P3 Instances have up to 8 NVIDIA Tesla V100 GPUs. Amazon EC2 P2 Instances have up to 16 NVIDIA NVIDIA K80 GPUs. Amazon EC2 G3 Instances have up to 4 NVIDIA Tesla M60 GPUs. Amazon EC2 G4 Instances have up to 4 NVIDIA T4 GPUs. Amazon EC2 P4 Instances have up to 8 NVIDIA Tesla A100 GPUs. Check out EC2 Instance Types and choose Accelerated Computing to see the different GPU instance options. DLAMI instances provide tooling to monitor and optimize your GPU processes. For more information on overseeing your GPU processes, see GPU Monitoring and Optimization . For pricing see On Demand Pricing and Spot Pricing . Configure Instance Details Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 70% discount compared to On-Demand prices. We recommend a persistent spot instance, which will save your data and restart automatically when spot instance availability returns after spot instance termination. For full-price On-Demand instances leave these settings to their default values. Complete Steps 4-7 to finalize your instance hardware and security settings and then launch the instance. 3. Connect to Instance Select the check box next to your running instance, and then click connect. You can copy paste the SSH terminal command into a terminal of your choice to connect to your instance. 4. Run YOLOv5 \ud83d\ude80 Once you have logged in to your instance, clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies Then get started training, testing and detecting! $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos Optional Extras Add 64GB of swap memory (to --cache large datasets). sudo fallocate -l 64G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile free -h # check memory","title":"AWS Quickstart"},{"location":"tutorials/articles/AWS-Quickstart/#1-console-sign-in","text":"Create and account or sign-in to the AWS console at https://aws.amazon.com/console/ and then select the EC2 service.","title":"1. Console Sign-in"},{"location":"tutorials/articles/AWS-Quickstart/#2-launch-instance","text":"In the EC2 part of the AWS console, click the Launch instance button.","title":"2. Launch Instance"},{"location":"tutorials/articles/AWS-Quickstart/#choose-an-amazon-machine-image-ami","text":"Enter 'Deep Learning' in the search field and select the most recent Ubuntu Deep Learning AMI (recommended), or select an alternative Deep Learning AMI. See Choosing Your DLAMI for more information on selecting an AMI.","title":"Choose an Amazon Machine Image (AMI)"},{"location":"tutorials/articles/AWS-Quickstart/#select-an-instance-type","text":"A GPU instance is recommended for most deep learning purposes. Training new models will be faster on a GPU instance than a CPU instance. You can scale sub-linearly when you have multi-GPU instances or if you use distributed training across many instances with GPUs. To set up distributed training, see Distrbuted Training . Note: The size of your model should be a factor in selecting an instance. If your model exceeds an instance's available RAM, select a different instance type with enough memory for your application. Amazon EC2 P3 Instances have up to 8 NVIDIA Tesla V100 GPUs. Amazon EC2 P2 Instances have up to 16 NVIDIA NVIDIA K80 GPUs. Amazon EC2 G3 Instances have up to 4 NVIDIA Tesla M60 GPUs. Amazon EC2 G4 Instances have up to 4 NVIDIA T4 GPUs. Amazon EC2 P4 Instances have up to 8 NVIDIA Tesla A100 GPUs. Check out EC2 Instance Types and choose Accelerated Computing to see the different GPU instance options. DLAMI instances provide tooling to monitor and optimize your GPU processes. For more information on overseeing your GPU processes, see GPU Monitoring and Optimization . For pricing see On Demand Pricing and Spot Pricing .","title":"Select an Instance Type"},{"location":"tutorials/articles/AWS-Quickstart/#configure-instance-details","text":"Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 70% discount compared to On-Demand prices. We recommend a persistent spot instance, which will save your data and restart automatically when spot instance availability returns after spot instance termination. For full-price On-Demand instances leave these settings to their default values. Complete Steps 4-7 to finalize your instance hardware and security settings and then launch the instance.","title":"Configure Instance Details"},{"location":"tutorials/articles/AWS-Quickstart/#3-connect-to-instance","text":"Select the check box next to your running instance, and then click connect. You can copy paste the SSH terminal command into a terminal of your choice to connect to your instance.","title":"3. Connect to Instance"},{"location":"tutorials/articles/AWS-Quickstart/#4-run-yolov5","text":"Once you have logged in to your instance, clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies Then get started training, testing and detecting! $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos","title":"4. Run YOLOv5 \ud83d\ude80"},{"location":"tutorials/articles/AWS-Quickstart/#optional-extras","text":"Add 64GB of swap memory (to --cache large datasets). sudo fallocate -l 64G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile free -h # check memory","title":"Optional Extras"},{"location":"tutorials/articles/Docker-Quickstart/","text":"To get started with YOLOv5 \ud83d\ude80 in a Docker image follow the instructions below. Other quickstart options for YOLOv5 include our Colab Notebook and a GCP Deep Learning VM . 1. Install Docker and Nvidia-Docker Docker images come with all dependencies preinstalled, however Docker itself requires installation, and relies of nvidia driver installations in order to interact properly with local GPU resources. The requirements are: - Nvidia Driver >= 455.23 https://www.nvidia.com/Download/index.aspx - Nvidia-Docker https://github.com/NVIDIA/nvidia-docker - Docker Engine - CE >= 19.03 https://docs.docker.com/install/ 2. Pull Image The Ultralytics YOLOv5 DockerHub is https://hub.docker.com/r/ultralytics/yolov5 . Docker Autobuild is used to automatically build images from the latest repository commits, so the ultralytics/yolov5:latest image hosted on the DockerHub will always be in sync with the most recent repository commit . To pull this image: sudo docker pull ultralytics/yolov5:latest 3. Run Container Run an interactive instance of this image (called a \"container\") using -it : sudo docker run --ipc=host -it ultralytics/yolov5:latest Run a container with local file access (like COCO training data in /coco ) using -v : sudo docker run --ipc=host -it -v \"$(pwd)\"/coco:/usr/src/coco ultralytics/yolov5:latest Run a container with GPU access using --gpus all : sudo docker run --ipc=host --gpus all -it ultralytics/yolov5:latest 4. Run Commands Run commands from within the running Docker container, i.e.: $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos","title":"Docker Quickstart"},{"location":"tutorials/articles/Docker-Quickstart/#1-install-docker-and-nvidia-docker","text":"Docker images come with all dependencies preinstalled, however Docker itself requires installation, and relies of nvidia driver installations in order to interact properly with local GPU resources. The requirements are: - Nvidia Driver >= 455.23 https://www.nvidia.com/Download/index.aspx - Nvidia-Docker https://github.com/NVIDIA/nvidia-docker - Docker Engine - CE >= 19.03 https://docs.docker.com/install/","title":"1. Install Docker and Nvidia-Docker"},{"location":"tutorials/articles/Docker-Quickstart/#2-pull-image","text":"The Ultralytics YOLOv5 DockerHub is https://hub.docker.com/r/ultralytics/yolov5 . Docker Autobuild is used to automatically build images from the latest repository commits, so the ultralytics/yolov5:latest image hosted on the DockerHub will always be in sync with the most recent repository commit . To pull this image: sudo docker pull ultralytics/yolov5:latest","title":"2. Pull Image"},{"location":"tutorials/articles/Docker-Quickstart/#3-run-container","text":"Run an interactive instance of this image (called a \"container\") using -it : sudo docker run --ipc=host -it ultralytics/yolov5:latest Run a container with local file access (like COCO training data in /coco ) using -v : sudo docker run --ipc=host -it -v \"$(pwd)\"/coco:/usr/src/coco ultralytics/yolov5:latest Run a container with GPU access using --gpus all : sudo docker run --ipc=host --gpus all -it ultralytics/yolov5:latest","title":"3. Run Container"},{"location":"tutorials/articles/Docker-Quickstart/#4-run-commands","text":"Run commands from within the running Docker container, i.e.: $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos","title":"4. Run Commands"},{"location":"tutorials/articles/GCP-Quickstart/","text":"This quickstart guide helps new users run YOLOv5 \ud83d\ude80 on a Google Cloud Platform (GCP) Deep Learning Virtual Machine (VM) \u2b50. New GCP users are eligible for a $300 free credit offer . Other quickstart options for YOLOv5 include our Colab Notebook and our Docker image at https://hub.docker.com/r/ultralytics/yolov5 . 1. Create VM Select a Deep Learning VM from the GCP marketplace , select an n1-standard-8 instance (with 8 vCPUs and 30 GB memory), add a GPU of your choice, check 'Install NVIDIA GPU driver automatically on first startup?', and select a 300 GB SSD Persistent Disk for sufficient I/O speed, then click 'Deploy'. All dependencies are included in the preinstalled Anaconda Python environment. 2. Setup VM Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies 3. Run Commands $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos Optional Extras Add 64GB of swap memory (to --cache large datasets). sudo fallocate -l 64G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile free -h # check memory Mount local SSD lsblk sudo mkfs.ext4 -F /dev/nvme0n1 sudo mkdir -p /mnt/disks/nvme0n1 sudo mount /dev/nvme0n1 /mnt/disks/nvme0n1 sudo chmod a+w /mnt/disks/nvme0n1 cp -r coco /mnt/disks/nvme0n1","title":"GCP Quickstart"},{"location":"tutorials/articles/GCP-Quickstart/#1-create-vm","text":"Select a Deep Learning VM from the GCP marketplace , select an n1-standard-8 instance (with 8 vCPUs and 30 GB memory), add a GPU of your choice, check 'Install NVIDIA GPU driver automatically on first startup?', and select a 300 GB SSD Persistent Disk for sufficient I/O speed, then click 'Deploy'. All dependencies are included in the preinstalled Anaconda Python environment.","title":"1. Create VM"},{"location":"tutorials/articles/GCP-Quickstart/#2-setup-vm","text":"Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies","title":"2. Setup VM"},{"location":"tutorials/articles/GCP-Quickstart/#3-run-commands","text":"$ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos","title":"3. Run Commands"},{"location":"tutorials/articles/GCP-Quickstart/#optional-extras","text":"Add 64GB of swap memory (to --cache large datasets). sudo fallocate -l 64G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile free -h # check memory Mount local SSD lsblk sudo mkfs.ext4 -F /dev/nvme0n1 sudo mkdir -p /mnt/disks/nvme0n1 sudo mount /dev/nvme0n1 /mnt/disks/nvme0n1 sudo chmod a+w /mnt/disks/nvme0n1 cp -r coco /mnt/disks/nvme0n1","title":"Optional Extras"},{"location":"tutorials/articles/github/","text":"one dsfsdf two dsfsdf sdfdsf dfsdsf dsfdsf dsfdsf sdfsd dsfsd sdf sdf sdf sdf sdf sdf sdf sdf test","title":"GitHub"},{"location":"tutorials/articles/github/#one","text":"dsfsdf","title":"one"},{"location":"tutorials/articles/github/#two","text":"dsfsdf sdfdsf dfsdsf dsfdsf dsfdsf sdfsd dsfsd sdf sdf sdf sdf sdf sdf sdf sdf","title":"two"},{"location":"tutorials/articles/github/#test","text":"","title":"test"},{"location":"tutorials/articles/train-custom-models/","text":"Back to Tutorials Train Custom Models Learn how to train your own custom dataset with YOLOv5 and create a custom model which can later be exported and used in your application. Before You Start Ensure you have cloned the repository and installed all the required packages. Read the Quick Start Guide for instructions Train On Custom Data 1. Create dataset.yaml COCO128 is a small tutorial dataset composed of the first 128 images in COCO train2017. These same 128 images are used for both training and validation to verify our training pipeline is capable of overfitting. data/coco128.yaml , shown below, is the dataset configuration file that defines 1) an optional download command/URL for auto-downloading, 2) a path to a directory of training images (or path to a *.txt file with a list of training images), 3) the same for our validation images, 4) the number of classes, 5) a list of class names: # download command/URL (optional) download: https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip # train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/] train: ../coco128/images/train2017/ val: ../coco128/images/train2017/ # number of classes nc: 80 # class names names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] 2. Create Labels After using a tool like CVAT , makesense.ai or Labelbox to label your images, export your labels to YOLO format , with one *.txt file per image (if no objects in image, no *.txt file is required). The *.txt file specifications are: One row per object Each row is class x_center y_center width height format. Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height. Class numbers are zero-indexed (start from 0). The label file corresponding to the above image contains 2 persons (class 0 ) and a tie (class 27 ): 3. Organize Directories Organize your train and val images and labels according to the example below. In this example we assume /coco128 is next to the /yolov5 directory. YOLOv5 locates labels automatically for each image by replacing the last instance of /images/ in each image path with /labels/ . For example: dataset/images/im0.jpg # image dataset/labels/im0.txt # label 4. Select a Model Select a pretrained model to start training from. Here we select YOLOv5s , the smallest and fastest model available. See our README table for a full comparison of all models. 5. Train Train a YOLOv5s model on COCO128 by specifying dataset, batch-size, image size and either pretrained --weights yolov5s.pt (recommended), or randomly initialized --weights '' --cfg yolov5s.yaml (not recommended). Pretrained weights are auto-downloaded from the latest YOLOv5 release . # Train YOLOv5s on COCO128 for 5 epochs $ python train.py --img 640 --batch 16 --epochs 5 --data coco128.yaml --weights yolov5s.pt All training results are saved to runs/train/ with incrementing run directories, i.e. runs/train/exp2 , runs/train/exp3 etc. For more details see the Training section of our Google Colab Notebook. Visualize Weights & Biases Logging (\ud83d\ude80 NEW) Weights & Biases (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration among team members. To enable W&B logging install wandb , and then train normally (you will be guided setup on first use). $ pip install wandb During training you will see live updates at https://wandb.ai , and you can create Detailed Reports of your results using the W&B Reports tool. Local Logging All results are logged by default to runs/train , with a new experiment directory created for each new training as runs/train/exp2 , runs/train/exp3 , etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a Mosaic Dataloader is used for training (shown below), a new concept developed by Ultralytics and first featured in YOLOv4 . train_batch0.jpg shows train batch 0 mosaics and labels: test_batch0_labels.jpg shows test batch 0 labels: test_batch0_pred.jpg shows test batch 0 predictions : Training losses and performance metrics are also logged to Tensorboard and a custom results.txt logfile which is plotted as results.png (below) after training completes. Here we show YOLOv5s trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained --weights yolov5s.pt (orange). from utils.plots import plot_results plot_results(save_dir='runs/train/exp') # plot results.txt as results.png Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Train Custom Models"},{"location":"tutorials/articles/train-custom-models/#train-custom-models","text":"Learn how to train your own custom dataset with YOLOv5 and create a custom model which can later be exported and used in your application.","title":"Train Custom Models"},{"location":"tutorials/articles/train-custom-models/#before-you-start","text":"Ensure you have cloned the repository and installed all the required packages. Read the Quick Start Guide for instructions","title":"Before You Start"},{"location":"tutorials/articles/train-custom-models/#train-on-custom-data","text":"","title":"Train On Custom Data"},{"location":"tutorials/articles/train-custom-models/#1-create-datasetyaml","text":"COCO128 is a small tutorial dataset composed of the first 128 images in COCO train2017. These same 128 images are used for both training and validation to verify our training pipeline is capable of overfitting. data/coco128.yaml , shown below, is the dataset configuration file that defines 1) an optional download command/URL for auto-downloading, 2) a path to a directory of training images (or path to a *.txt file with a list of training images), 3) the same for our validation images, 4) the number of classes, 5) a list of class names: # download command/URL (optional) download: https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip # train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/] train: ../coco128/images/train2017/ val: ../coco128/images/train2017/ # number of classes nc: 80 # class names names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']","title":"1. Create dataset.yaml"},{"location":"tutorials/articles/train-custom-models/#2-create-labels","text":"After using a tool like CVAT , makesense.ai or Labelbox to label your images, export your labels to YOLO format , with one *.txt file per image (if no objects in image, no *.txt file is required). The *.txt file specifications are: One row per object Each row is class x_center y_center width height format. Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height. Class numbers are zero-indexed (start from 0). The label file corresponding to the above image contains 2 persons (class 0 ) and a tie (class 27 ):","title":"2. Create Labels"},{"location":"tutorials/articles/train-custom-models/#3-organize-directories","text":"Organize your train and val images and labels according to the example below. In this example we assume /coco128 is next to the /yolov5 directory. YOLOv5 locates labels automatically for each image by replacing the last instance of /images/ in each image path with /labels/ . For example: dataset/images/im0.jpg # image dataset/labels/im0.txt # label","title":"3. Organize Directories"},{"location":"tutorials/articles/train-custom-models/#4-select-a-model","text":"Select a pretrained model to start training from. Here we select YOLOv5s , the smallest and fastest model available. See our README table for a full comparison of all models.","title":"4. Select a Model"},{"location":"tutorials/articles/train-custom-models/#5-train","text":"Train a YOLOv5s model on COCO128 by specifying dataset, batch-size, image size and either pretrained --weights yolov5s.pt (recommended), or randomly initialized --weights '' --cfg yolov5s.yaml (not recommended). Pretrained weights are auto-downloaded from the latest YOLOv5 release . # Train YOLOv5s on COCO128 for 5 epochs $ python train.py --img 640 --batch 16 --epochs 5 --data coco128.yaml --weights yolov5s.pt All training results are saved to runs/train/ with incrementing run directories, i.e. runs/train/exp2 , runs/train/exp3 etc. For more details see the Training section of our Google Colab Notebook.","title":"5. Train"},{"location":"tutorials/articles/train-custom-models/#visualize","text":"","title":"Visualize"},{"location":"tutorials/articles/train-custom-models/#weights-biases-logging-new","text":"Weights & Biases (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration among team members. To enable W&B logging install wandb , and then train normally (you will be guided setup on first use). $ pip install wandb During training you will see live updates at https://wandb.ai , and you can create Detailed Reports of your results using the W&B Reports tool.","title":"Weights &amp; Biases Logging (\ud83d\ude80 NEW)"},{"location":"tutorials/articles/train-custom-models/#local-logging","text":"All results are logged by default to runs/train , with a new experiment directory created for each new training as runs/train/exp2 , runs/train/exp3 , etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a Mosaic Dataloader is used for training (shown below), a new concept developed by Ultralytics and first featured in YOLOv4 . train_batch0.jpg shows train batch 0 mosaics and labels: test_batch0_labels.jpg shows test batch 0 labels: test_batch0_pred.jpg shows test batch 0 predictions : Training losses and performance metrics are also logged to Tensorboard and a custom results.txt logfile which is plotted as results.png (below) after training completes. Here we show YOLOv5s trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained --weights yolov5s.pt (orange). from utils.plots import plot_results plot_results(save_dir='runs/train/exp') # plot results.txt as results.png","title":"Local Logging"},{"location":"tutorials/articles/train-custom-models/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/articles/train-custom-models/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/articles/training-tips-best-results/","text":"\ud83d\udc4b Hello! This is an advanced guide for achieving the best real-world results when training YOLOv5 \ud83d\ude80 (first-time users should start with the Train Custom Data Tutorial ). These are recommendations and not requirements, but the closer these recommendations are followed the higher the likelihood that you will be happy with your training results \ud83d\ude03. Most of the time good results can be obtained with no changes to the models or training settings, provided your dataset is sufficiently large and well labelled . If at first you don't get good results, there are steps you might be able to take to improve, but we always recommend users first train with all default settings before considering any changes. This helps establish a performance baseline and spot areas for improvement. If you start a discussion or raise an issue about your training results we recommend you provide the maximum amount of information possible for the best community response, including results plots (train losses, val losses, P, R, mAP), PR curve, confusion matrix, training mosaics, test results and dataset statistics such as labels.png. All of these are located in your project/name directory, typically yolov5/runs/train/exp . Dataset Images per class. \u22651.5k images per class Instances per class. \u226510k instances (labeled objects) per class total Image variety. Must be representative of deployed environment. For real-world use cases we recommend images from different times of day, different seasons, different weather, different lighting, different angles, different sources (scraped online, collected locally, different cameras) etc. Label consistency. All instances of all classes in all images must be labelled. Partial labelling will not work. Label accuracy. Labels must closely enclose each object. No space should exist between an object and it's bounding box. No objects should be missing a label. Background images. Background images are images with no objects that are added to a dataset to reduce False Positives (FP). We recommend about 0-10% background images to help reduce FPs (COCO has 1000 background images for reference, 1% of the total). Model Selection Larger models like YOLOv5x will produce better results in nearly all cases, but have more parameters and are slower to run. For mobile applications we recommend YOLOv5s/m, for cloud or desktop applications we recommend YOLOv5l/x. See our README table for a full comparison of all models. To start training from pretrained weights simply pass the name of the model to the --weights argument. Models download automatically from the latest YOLOv5 release . python train.py --data custom.yaml --weights yolov5s.pt yolov5m.pt yolov5l.pt yolov5x.pt Training Settings Before modifying anything, first train with default settings to establish a performance baseline . A full list of train.py settings can be found in the train.py argparser. Epochs. Start with 300 epochs. If this overfits early then you can reduce epochs. If overfitting does not occur after 300 epochs, train longer, i.e. 600, 1200 etc epochs. Image size. COCO trains at native resolution of --img 640 , though due to the high amount of small objects in the dataset it can benefit from training at higher resolutions such as --img 1280 . If there are many small objects then custom datasets will benefit from training at native or higher resolution. Best inference results are obtained at the same --img as the training was run at, i.e. if you train at --img 1280 you should also test and detect at --img 1280 . Batch size. Use the largest --batch-size that your hardware allows for. Small batch sizes produce poor batchnorm statistics and should be avoided. Hyperparameters. Default hyperparameters are in hyp.scratch.yaml . We recommend you train with default hyperparameters first before thinking of modifying any. In general, increasing augmentation hyperparameters will reduce and delay overfitting, allowing for longer trainings and higher final mAP. Reduction in loss component gain hyperparameters like hyp['obj'] will help reduce overfitting in those specific loss components. For an automated method of optimizing these hyperparameters, see our Hyperparameter Evolution Tutorial . Further Reading If you'd like to know more a good place to start is Karpathy's 'Recipe for Training Neural Networks', which has great ideas for training that apply broadly across ML domains: http://karpathy.github.io/2019/04/25/recipe/ Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Training tips best results"},{"location":"tutorials/articles/training-tips-best-results/#dataset","text":"Images per class. \u22651.5k images per class Instances per class. \u226510k instances (labeled objects) per class total Image variety. Must be representative of deployed environment. For real-world use cases we recommend images from different times of day, different seasons, different weather, different lighting, different angles, different sources (scraped online, collected locally, different cameras) etc. Label consistency. All instances of all classes in all images must be labelled. Partial labelling will not work. Label accuracy. Labels must closely enclose each object. No space should exist between an object and it's bounding box. No objects should be missing a label. Background images. Background images are images with no objects that are added to a dataset to reduce False Positives (FP). We recommend about 0-10% background images to help reduce FPs (COCO has 1000 background images for reference, 1% of the total).","title":"Dataset"},{"location":"tutorials/articles/training-tips-best-results/#model-selection","text":"Larger models like YOLOv5x will produce better results in nearly all cases, but have more parameters and are slower to run. For mobile applications we recommend YOLOv5s/m, for cloud or desktop applications we recommend YOLOv5l/x. See our README table for a full comparison of all models. To start training from pretrained weights simply pass the name of the model to the --weights argument. Models download automatically from the latest YOLOv5 release . python train.py --data custom.yaml --weights yolov5s.pt yolov5m.pt yolov5l.pt yolov5x.pt","title":"Model Selection"},{"location":"tutorials/articles/training-tips-best-results/#training-settings","text":"Before modifying anything, first train with default settings to establish a performance baseline . A full list of train.py settings can be found in the train.py argparser. Epochs. Start with 300 epochs. If this overfits early then you can reduce epochs. If overfitting does not occur after 300 epochs, train longer, i.e. 600, 1200 etc epochs. Image size. COCO trains at native resolution of --img 640 , though due to the high amount of small objects in the dataset it can benefit from training at higher resolutions such as --img 1280 . If there are many small objects then custom datasets will benefit from training at native or higher resolution. Best inference results are obtained at the same --img as the training was run at, i.e. if you train at --img 1280 you should also test and detect at --img 1280 . Batch size. Use the largest --batch-size that your hardware allows for. Small batch sizes produce poor batchnorm statistics and should be avoided. Hyperparameters. Default hyperparameters are in hyp.scratch.yaml . We recommend you train with default hyperparameters first before thinking of modifying any. In general, increasing augmentation hyperparameters will reduce and delay overfitting, allowing for longer trainings and higher final mAP. Reduction in loss component gain hyperparameters like hyp['obj'] will help reduce overfitting in those specific loss components. For an automated method of optimizing these hyperparameters, see our Hyperparameter Evolution Tutorial .","title":"Training Settings"},{"location":"tutorials/articles/training-tips-best-results/#further-reading","text":"If you'd like to know more a good place to start is Karpathy's 'Recipe for Training Neural Networks', which has great ideas for training that apply broadly across ML domains: http://karpathy.github.io/2019/04/25/recipe/","title":"Further Reading"},{"location":"tutorials/articles/training-tips-best-results/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/articles/training-tips-best-results/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"}]}